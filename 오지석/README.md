### 목차

1. [250304\_화](#250304_화)
2. [250305\_수](#250305_수)
3. [250306\_목](#250306_목)

# 250304\_화

---

- 기획 구체화 및 설계 관련 논의

  - hadoop 분산 처리를 어떻게 할 것 인지 설계

    1. 경제 관련 기사를 크롤링하여 매일 단위로 잘라서 저장
    2. hdfs로 불러와서 다시 블록 단위로 분산
    3. spark와 newspaper3k로 전처리
    4. hugging face를 활용해서 문단 단위 감성 분석

  - cassandra db를 활용해서 트랜잭션을 저장할 수 있을까?

    - 장점: 특정 계좌에 대한 대출과 상환 트랜잭션 로그를 한번의 조회로 할 수 있고 append에 유리한 구조이기 때문에 성능적 이점
    - 단점: 어차피 rdbms를 사용하는 상황에서 추가적인 데이터에 대한 관리가 필요, ACID를 완벽히 지키기 어려울 수 있음
    - 결론: 조회의 편리함과 삽입에 대해 약간의 성능적 이점에 비해 데이터를 둘로 나눠 관리하면서 생기는 단점이 더 크다고 판단, nosql에 대해 학습을 하고 싶다면 로깅을 하는 과정에서 쓰는 쪽으로 정리

  - 미팅 후 느낀 부족한 부분
    - 기획의 내용을 알기 쉽게 전달하기 어려움 -> 대출과 상환 프로세스에 대한 구체화 필요
    - 각종 용어들에 대한 통일과 도식화 필요
    - kafka 용처에 대한 고민: 대출 과정에서 하나의 대출 요청에 대해 분산처리하는 과정에서 사용? -> 어떻게 사용할 지 고민이 필요

# 250305\_수

- 크롤링 성능 향상 방법 논의
  - 기존 크롤링 코드는 크게 세부분으로 나눌 수 있는데
    1. 네이버 검색을 통해 기사의 링크를 수집
    2. 각 링크에 대해 http 요청을 보내고 제목과 본문을 추출
    3. 일일 단위 결과 파일에 저장
  - 이는 1일 약 1600 ~ 1700개 정도의 기사를 처리하는데 50분 정도가 소요됨
  - 엄밀히 2번과 3번은 코드적으로는 묶여 있지만 병렬 처리를 위해서 분리하는 것이 맞다고 판단
  - 2번 파트만 병렬적으로 처리하고 해당 처리가 완료되면 파일에 씀으로써 파일 출력이 뒤섞이지 않도록 설계
  - 결과적으로 1일당 50분에서 10분까지 시간 소요를 줄일 수 있었음
- 기획 구체화
  - MVP 기준 기획 구체화
    - 목표 수익률을 초기 자금 예치시 설정한 값으로 고정
      - 해당 목표 슈익률 기준 실제 수익률 별 누적 횟수 분포 그래프용 데이터 필요
    - 투자내역
      - 투자 기간을 설정해서 그 기간 동안의 이자 수익과 예상 수익 표현
      - 이자 수익: 이미 수취한 이자만 표현
      - 예상 수익: 현재까지 수익금을 구하고 남은 기간에 대해 목표 수익률의 신뢰구간내 최솟값 최댓값 평균값을 구해 더한 값을 보여준다
    - 고정 금리
    - 상환 방식(만기일 일시 상환)
  - 수익률 설정 방법
    - 이자율 = 무위험이자율 + 신용위험 프리미엄 + 기타 비용 + 마진
    - 신용위험 프리미엄 = 부도율 \* 손실률 / (1 - 부도율)
  - 추후 추가할만한 부분들을 미리 고려해두고 해당 부분을 갈아낄 수 있도록 추상화 고려

# 250306\_목

- 설계 구체화

  - 카프카
    - 대출 심사 과정에서 신용 정보, 소득 및 재직 정보, 거래 내역 및 자산 정보 등 다양한 외부 서비스에 대한 호출이 필요
    - 해당 부분들을 병렬로 비동기 처리하면서 결합도를 떨어뜨리기 위해 카프카를 사용
    - 대출 실행 부분에서 시간이 오래 걸릴 수 있다면 비동기 처리가 필요할 수 있다.
    - 대출 실행 부분은 정확히 한번 실행되는 것이 중요한 이슈이다. 이 때 정확히 한번이라는 조건은 들리는 것 보다 훨씬 복잡한 문제였다. [Exactly Once](https://velog.io/@xogml951/Kafka%EC%99%80-Exactly-Once)
    - 장애 시 재처리를 고려 했을 때도 다른 비동기 처리 방식에 비해 장점이 있는 것으로 보인다.
  - 통계 관련

    - 통계 데이터를 대출 트랜잭션이 끝나면 실패를 신경 쓰지 않는 통계 서비스를 실행 시킨다.
    - 추후 일일 스케쥴러가 이전에 쌓여있던 통계치를 무시하고(실패한 측정을 고려) 다시 검산해서 업데이트

  - 로그 관련(우선도 낮음)
    - 각 요청들에 대한 로깅
    - 요청마다 traceId를 설정해주고 다른 MSA로 넘어갈 때 해당 traceId 유지하도록 구현
